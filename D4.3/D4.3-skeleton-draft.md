**Table of Contents**  *generated with [DocToc](http://doctoc.herokuapp.com/)*

- [Evaluation framework](#evaluation-framework)
- [Checklist service](#checklist-service)
	- [Component description](#component-description)
	- [Usability study](#usability-study)
		- [User perspective](#user-perspective)
			- [Survey](#survey)
		- [User-developer perspective](#user-developer-perspective)
			- [Survey](#survey-1)
		- [Developer perspective](#developer-perspective)
			- [Survey](#survey-2)
		- [Summary of usability results](#summary-of-usability-results)
	- [Benchmarking](#benchmarking)
		- [Capabilities](#capabilities)
		- [Performance](#performance)
	- [Sustainability and maintainability study](#sustainability-and-maintainability-study)
- [`mkminim` Checklist Creator](#mkminim-checklist-creator)
- [Stability service](#stability-service)
- [Component C](#component-c)
	- [Component description](#component-description-1)
	- [Usability study](#usability-study-1)
		- [End user perspective](#end-user-perspective)
			- [End-user characterization](#end-user-characterization)
			- [Task descriptions](#task-descriptions)
			- [Task accomplishment](#task-accomplishment)
		- [User-developer perspective](#user-developer-perspective-1)
			- [User-developer characterization](#user-developer-characterization)
			- [Task descriptions](#task-descriptions-1)
			- [Task accomplishment](#task-accomplishment-1)
		- [Developer perspective](#developer-perspective-1)
			- [Developer characterization](#developer-characterization)
			- [Task descriptions](#task-descriptions-2)
			- [Task accomplishment](#task-accomplishment-2)
		- [Summary of results](#summary-of-results)
	- [Benchmarking](#benchmarking-1)
	- [Sustainability and maintainability study](#sustainability-and-maintainability-study-1)
- [Conclusions](#conclusions)

# Notes/questions (temporary section)

<!--
* Identify WP4 components to report.
* thrash out extent of usability testing, need to limit scope of work.  
* I have no more than 15 working days available by the time the delicverable draft needs to be available for QA.  I expect a substantial amount of that time to be taken up with writing and organizing the material.  Also, I don't have access to candidate users to test with.
-->

# Evaluation framework

The evaluation of the components is based on the guidelines provided by the UK's Software Sustainability Institute (SSI):[http://www.software.ac.uk/software-evaluation-guide](), [http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationTutorial.pdf]() and [http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationCriteria.pdf]()

SSI provides two types of software evaluation approaches. One of them based on specific tasks to be accomplished by the user, who reports on the experience running those tasks, and another one based on a number or criteria which are checked or not, depending on the qualtiy and status of the software.

For user-facing software components, a usability study should be performed.  Most of the WP4 components are fronted by other parts of the system (myExperiment, RO Portal, etc.), and as such have not been subjected to the same level of usability studies as other parts

Our proposal is to apply the former to conduct a usability study of the components and the latter to measure the sustainability and maintanibility of the components, especially after the project. Please note this will probably highlight pitfalls in our software that we will need to address before project's end(we don't want to release components with identified problems). A third dimension to be included in the evaluation is some benchmarkin of the components in order to measure indicators like performance, coverage, time, cost, scalability, etc.


# Checklist service

## Component description

The checklist service takes an RO, a Minim checklist description and other parameters, and on the basis of these performs an evaluation of the RO or specified resource and returns a result indicating how well the requirements of the checklist were satisfied.

See also:
* (D4.2)
* [http://www.wf4ever-project.org/wiki/display/docs/RO+decay+detection+using+checklists]()
* [http://www.wf4ever-project.org/wiki/display/docs/RO+checklist+evaluation+API]()
* [http://www.wf4ever-project.org/wiki/display/docs/Checklist+traffic+light+API]()

## Usability study

* Web links:
  * [https://github.com/wf4ever/ro-manager]()
  * [https://pypi.python.org/pypi/ro-manager]()
  * @@TODO need page for checklist service installation, etc.
  * [http://www.wf4ever-project.org/wiki/display/docs/RO+Manager+FAQ]()
  * [http://www.wf4ever-project.org/wiki/display/docs/Sandbox+configuration]()

### User perspective

> User perspective relates to usability by an end-user of the software

This component is not used directly by a user.  Rather, it is called by other user-facing Wf4Ever components to provide information about how well RO meets some designated criteria.  As such, it has not been subjected to a separate usability study.

#### Survey

(Based on [SSI evaluation guide](http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationTutorial.pdf))

General usability: **N/A**

* Visibility of system status. Does it give users appropriate feedback within reasonable time?
* Match between system and the real world. Does it speak the user’s language and make information appear in a natural and logical order? Are implementation-specific details hidden from the user?
* User control and freedom. Does it provide clearly marked exits, undo and redo?
* Consistency and standards. Is it consistent within the software, with other similar packages and with platform conventions?
* Error prevention. Does it prevent errors in the first place or help users avoid making them?
* Recognition rather than recall. Does it make objects, actions and options visible and reduce the amount of information a user has to remember?
* Flexibility and efficiency of use. Does it offer short-cuts and support macros for frequently- done action sequences?
* Aesthetic and minimalist design. Does it avoid showing irrelevant or rarely-needed information?
* Help users recognize, diagnose, and recover from errors. Does it make errors clear, comprehensible, precise and suggest solutions if possible?
* Help and documentation. Does it provide concise, accurate, clear, easily-searchable task- oriented doc centred around concrete lists of steps?
* Robustness.  Are responses sensible when instructions are not followed; e.g., wrong commands, illegal values, etc.?
* Obviousness. Can tasks be accomplished without any consultation of user documents or other material?

Release packaging:

* Are the binary releases packaged for immediate use in a suitable archive format?
  * Yes: installation via [PyPI](https://pypi.python.org/pypi/ro-manager)
  * @@TODO: checklist deployment documentation; also needs Pyramid
* Is it clear how to get the software from the web site? Does it have version numbers?
  * Yes
* Is it clear from the web site or user doc what other packages are required?
  * @@TODO - make pyramid dependency clear 
* Is it clear what the licencing and copyright is on the web site?
  * @@TODO - clarify copyright and MIT licensing
* How to get started.  Is there a README, FAQ?
  * @@TODO for checklist

User documentation:

* Is the user doc accurate?
  * Moderately @@TODO: could do with review and checking
* Does it partition user, user-developer and developer information or mix it all together?
  * Sme partitioning, but not rigorous
* Is the user doc online?
  * Yes (github and Wf4Ever wiki)
* Are there any supporting tutorials?
  * No
* Do these list the versions they apply to?
  * N/A
* Is it task-oriented, structured around helping users achieve their tasks?
  * N/A

Help and support:

* Is there a list of known bugs and issues, or a bug/issue tracker?
  * Sort-of: [https://github.com/wf4ever/ro-manager/issues](), [https://github.com/wf4ever/ro-manager/blob/master/TODO.txt]()
* Is it clear how to ask for help e.g. where to e-mail or how to enter bugs/issues
  * No.
* Are there e-mail list archives or forums?
  * No
* If so, is there evidence of use?
  * N/A
* Are they searchable?
  * N/A
* Is there a bug/issue tracker?
  * Yes (but not much used).
  * [https://github.com/wf4ever/ro-manager/issues]()
  * [https://github.com/wf4ever/ro-manager/blob/master/TODO.txt]()
  * [https://jira.man.poznan.pl/jira/issues]() (not public)
* If so, there evidence of use?
  * Some, but not consistent
* Does it seem that bugs and issues are resolved or, at least, looked at?
  * Some
* Is it clear what quality of service a user expect in terms of support e.g. best effort, reasonable effort, reply in 24 hours etc.?
  * No
* Is it clear how to contribute bugs, issues, corrections (e.g. in tutorials or user doc) or ideas?
  * No, but use of Github offers a generic route.


### User-developer perspective

> User-developer perspective relates to usability by developer who makes use of the software in som,e opther software component.

The checklist provides a simple [REST API](http://www.wf4ever-project.org/wiki/display/docs/RO+checklist+evaluation+API) which is used by other component developers to perform a required evaluation.  There is also a ["traffic light API"](http://www.wf4ever-project.org/wiki/display/docs/Checklist+traffic+light+API) that is very similar in style.  The REST API has proved quite easy to use, and has been successfully integrated with minimal additional guidance from the checklist service developer by other components of the Wf4Ever project:

1. [Showcase 47](http://www.wf4ever-project.org/wiki/pages/viewpage.action?pageId=3506198): in this activity, the checklist service (developed by Oxford) was accessed by an early prototype quality display service (developed separately by iSOCO).  This was out first attempt to integrate Integrity and Authenticity components developed by different project members, and showed that the REST style adopted could facilitate integration of software components.
2. [myExperiment](@@ref): the checklist display has been integrated into the displayt of a myExperiment PACK.
3. [RO Portal](@@ref): the checklist display has been integrated into the RO portal display of a Research Object.

To perform a checklist evaluation, a checklist description must be created if one does not already exis. This requires some knowledge of RDF and SPARQL.  Originally, checklists were created by hand-editing RDF, which in practice meant they were initially coded by the checklist software developer.  Once an initial checklist had been created, other developers were generally able to make modest changes to the checklist (based on experiences from setting up software deminstrations, e.g. [http://www.wf4ever-project.org/wiki/display/docs/135.+TIMBUS+Demo+preparation]()).

More recently, based in part on input from a new project member, we have designed a spreadsheet based format for creating checklists, and created a tool [mkminim](https://github.com/wf4ever/ro-manager/blob/master/src/checklist/mkminim.md) for converting this to RDF for consumption by the checklist evaluation service.  At the time of writing, we have not conducted a formal usability study of this tool and the associated spreadsheet format.

As part of another approach to mitigating the possible difficulty of creating checklist descriptions, we have created an in [initial collection of example and skeleton checklists](https://github.com/wf4ever/ro-catalogue/tree/master/minim), which may be used as a starting point for creating new checklist definitions.

#### Survey

(Based on [SSI evaluation guide](http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationTutorial.pdf))

How easy is it to set up development environment to write code that uses the software or service? This may involve getting the source code of the software but for online services, it might not.
* Is it clear what third-party tools and software you need, which versions you need, where to get these and how to set them up?
  * Mostly handled by PyPI
  * @@TODO install documentation for checklist service
* Are there tutorials available for user-developers?
  * No
* If so, Do these list the versions they apply to? Are they accurate, and understandable?
  * N/A
* Is there example code that can be compiled, customised and used?
  * Yes
  * @@TODO: locate and link
* How accurate, understandable and complete is the API documentation? Does it provide
examples of use?
  * [http://www.wf4ever-project.org/wiki/display/docs/RO+checklist+evaluation+API]()
  * [http://www.wf4ever-project.org/wiki/display/docs/Checklist+traffic+light+API]()
* For services, is there information about quality of service? e.g. number of requests that can be run in a specific time period. How do user-developers find out when services might be down etc.
  * No.
* For services, is there information about copyright and licencing as to how the services can be used? e.g. for non-commercial purposes only, does the project have to be credited etc. Is there information on how any data can be used, who owns it etc.?
  * No.
<!--
* Is there a contributions policy that allows user-developers to contribute their components to the client? How restrictive is it? Who owns the contributions?
  * @@Don't understand.  What is "Client": here?
-->?
* Is the copyright and licencing of the software and third-party dependencies clear and documented so you can understand the implications on extensions you write?
  * N/A


### Developer perspective

> Developer perspective relates to usability by a developer who is trying to change the software to improve or fix it

The checklist evaluation software has been developed as part of the RO Manager software suite, and makes use of many of the same components.  Source code is written in Python, and development has followed a test-led development practice, an effect of which is that there are many examples of the code function for other develiopers for study.  

Development of RO Manager has been led by a single programmer at Oxford, but part of the suite, handling exchange of Research Objects with RODL, has been written by developers at Poznan.  This gives us some confidence to claim that the code base is accessible and usable by developers who wish to enhance and/or fix the software.

#### Survey

(Based on [SSI evaluation guide](http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationTutorial.pdf))

* How easy is it to set up development environment to change the software?
  * Fairly easy - stahndard Python environment @@TODO but documentation needed for checklist service
  * NOTE: not tested under Windows.
* How easy is it to access to up-to-date versions of the source code that reflect changes made since the last release? i.e. access to the source code repository.
  * https://github.com/wf4ever/ro-manager
* How easy is it to understand the structure of the source code repository? Is there information that relates the structure of the source code to the software’s architecture?
  * @@TODO code structure overview in GitHub
* Is it clear what third-party tools and software you need, which versions you need, where to get these and how to set them up?
  * @@TODO install documentation
* How easy is it to compile the code?
  * Standard Python environment: no separate compilation needed.
* How easy is it to build a release bundle or deploy a service?
  * Easy (see https://github.com/wf4ever/ro-manager/blob/master/NOTES.txt)
* How easy is it to validate changes you’ve made? This includes building the software, getting, building and running tests.
  * Fairly easy
  * @@TODO install documentation
* Is there design documentation available? How accurate and understandable is it?
  * Some, not comprehensive
  * LISC paper
  * @@TODO code structure overview in GitHub
* Are there tutorials available for developers?
  * No
* If so, are they accurate, and understandable?
  * N/A
* How readable is the source code? Well-laid out with good use of white-space and
indentation?
  * OK
* How accurate or comprehensive is the source code commenting? Does it focus on why the code is as it is?
  * OK
* Is there a contributions policy that allows developers to contribute their changes to the client? How restrictive is it? Who owns the contributions?
  * No
* Is the copyright and licencing of the software and third-party dependencies clear and documented so you can understand the implications on changes you develop?
  * No
* For open source projects, is it clear how you could become a member?
  * No

### Summary of usability results

The checklist evaluation service is targeted at developers who wish to use the checklist service to provide quality indications of Research Objects in other components of workflow preservation software.  Our experiences indicate that the API is easy to use, and the code base is understandable and maintainable by third party developers.

The checklist creation aspects of this work need further work to make them accessible to end users; we have made some initial steps towards this goal.  The ease with which checklists have been adapted indicates that the existing tools are somewhat usable by developers with some knowledge of RDF. SPARQL and the RO Model.


## Benchmarking

> This section should cover everything related to functional and non functional aspects of the component, including performance, the coverage it provides, the time necessary to execute its regular operation, etc. Whenever possible a baseline should be established against which to compare.

### Capabilities

> See [http://www.wf4ever-project.org/wiki/display/docs/Showcase+128+-+Evaluate+checklist+toolkit]()

### Performance

> @@TODO

> See [https://github.com/wf4ever/ro-catalogue/blob/master/v0.1/minim-evaluation/performance-evaluation.md]()

## Sustainability and maintainability study

> [http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationCriteria.pdf]


and fill in the form for the sustainability and maintainability criteria

# `mkminim` Checklist Creator

# Stability service


# Component C

....


## Component description

> One subsection providing a sumary of each component to be evaluated and their role in the overall Wf4Ever infrastructure


## Usability study

> [http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationTutorial.pdf]()

> This aspect of the evaluation centres around carrying out typical tasks using the software. The nature of the tasks will depend upon the result to evaluate and they should be representative of the core functionality of the software and its intended purpose. Not all user&nbsp; types are relevant for each component. Identify yours and focus on those which are important. For example, middleware like RO transformation software should probably focus on user-developers and developers, while CollabSpheres should additionally include end-users. Same with models. Examples of end users, user-developers and developers could be for example: Astronomers and Biologists, Bio/astro informaticians, and a research engineer outsides Wf4Ever, respectively


### End user perspective

> Focus on ease of download, installation and use of software and models

#### End-user characterization


#### Task descriptions


#### Task accomplishment


### User-developer perspective

> Focus on development, programming components against published extensibility points and extending models


#### User-developer characterization


#### Task descriptions


#### Task accomplishment


### Developer perspective

> Focus on development related to changing the software and models to improve or fix them


#### Developer characterization


#### Task descriptions


#### Task accomplishment


### Summary of results


## Benchmarking

> This section should cover everything related to functional and non functional aspects of the component, including performance, the coverage it provides, the time necessary to execute its regular operation, etc. Whenever possible a baseline should be established against which to compare.


## Sustainability and maintainability study

> [http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationCriteria.pdf]


and fill in the form for the sustainability and maintainability criteria

...

# Conclusions