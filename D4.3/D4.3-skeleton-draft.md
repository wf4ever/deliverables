**Table of Contents**  *generated with [DocToc](http://doctoc.herokuapp.com/)*

- [Introduction](#introduction)
- [Evaluation framework](#evaluation-framework)
- [Checklist service](#checklist-service)
	- [Component description](#component-description)
	- [Usability study](#usability-study)
		- [User perspective](#user-perspective)
			- [Survey](#survey)
		- [User-developer perspective](#user-developer-perspective)
			- [Survey](#survey-1)
		- [Developer perspective](#developer-perspective)
			- [Survey](#survey-2)
	- [Benchmarking](#benchmarking)
		- [Capabilities](#capabilities)
		- [Performance](#performance)
	- [Sustainability and maintainability](#sustainability-and-maintainability)
	- [Summary of evaluation for checklist service](#summary-of-evaluation-for-checklist-service)
- [Stability service](#stability-service)
	- [Component description](#component-description-1)
	- [Usability study](#usability-study-1)
		- [User perspective](#user-perspective-1)
			- [Survey](#survey-3)
		- [User-developer perspective](#user-developer-perspective-1)
			- [Survey](#survey-4)
	- [Benchmarking](#benchmarking-1)
		- [Capabilities](#capabilities-1)
		- [Performance](#performance-1)
	- [Sustainability and maintainability](#sustainability-and-maintainability-1)
		- [Survey](#survey-5)
	- [Summary of evaluation for stability service](#summary-of-evaluation-for-stability-service)
- [Conclusions](#conclusions)
- [References](#references)

# Introduction

@@TODO: GK draft some blurb, pass to Jose for review and maybe expansion


# Evaluation framework

The evaluation of the components is structured around the guidelines provided by the UK's Software Sustainability Institute (SSI): [Software evaluation guide][SSI-guide], [Software Evaluation: Tutorial-based Assessment][SSI-tutorial] and [Software Evaluation: Criteria-based Assessment][SSI-criteria].

The SSI evaluation guides target three kinds of user: end-users (i.e. non-developers who will employ the software), developer-users (i.e., developers who will use APIs and other middleware facilities provided in the creation of some wider application) and developers (who are tasked with enhancing or repairing the software).

The guides further describe two software evaluation approaches.  Tutorial-based assessment is based on specific tasks to be accomplished by a user, who reports on the experience running those tasks, and Criteria-based assessment on a number or criteria which may be checked depending on the quality and status of the software.  There is a fair degree of overlap between the two evaluation approaches in the topics that they are intended to cover, which we have tried to minmimize in the adaptation of the SSI topic coverage reported here.

For user-facing software components, usability should be described.  The WP4 components are mostly middleware fronted by other parts of the system (such as myExperiment, RO Portal, etc.), and as such are not involved in the same level of direct user-facing deployment as those other components. The main focus of our usability evaluation is therefore on the developer-user aspects, and the ease and effectiveness of integration of our components with other parts of the Wf4Ever reference implementation.

For developers, usability focuses on the ease of accessing, building, understanding, enhancing and testing the code.  There is a strong connection between developer usability and overall sustainability and maintainability of a software product, reflected by some overlap in the topics assessed.

A third dimension to be included in the evaluation is some benchmarking of the components in order to assess indicators of functional capability and performance.

Finally, features of the software base and its management structure are surveyed to get a sense of its sustainability and maintainability, and what further activities might be needed to make the software into a fully fledged, sustainable product.  Many of the features covered here relate to the creation and sustenance of a community of users and developers for the product.

This framework is reflected in the structure of the sections that follow, which deal with evaluation of the checklist service and stability service respectively.  The overall evaluation reporting structure for each is as follows:

1. Component description, sufficient to place the evaluation within the overall context of the Wf4Ever reference architecture.

2. Usability study, reporting on applicable usability and effectiveness of the component from:

    * end-user (researcher) perspective
    * developer-user and system integration perspective
    * developer perspective, for enhancing and repairing the software

    The reporting is based on considerations covered by the SSI [Tutorial-based Assessment][SSI-tutorial],  adapted to draw upon our ongoing evaluation efforts, reflecting the agile nature of our development.  The survey sub-sections draw particularly from the SSI guide.
3. Benchmarking: an assessment of functional and performance characteristics of the software with respect to the tasks it is expected to perform.

4. Sustainability and maintainability study, based on the SSI [Criteria-based Assessment][SSI-criteria].

[SSI-guide]:    http://www.software.ac.uk/software-evaluation-guide (SSI software evaluation guide)
[SSI-tutorial]: http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationTutorial.pdf (Software Evaluation: Tutorial-based Assessment)
[SSI-criteria]: http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationCriteria.pdf (Software Evaluation: Criteria-based Assessment)


# Checklist service

@@TODO: GK this section

## Component description

The checklist service takes an RO, a Minim checklist description and other parameters, and on the basis of these performs an evaluation of the RO or specified resource and returns a result indicating how well the requirements of the checklist were satisfied.

See also:
* (D4.2)
* [http://www.wf4ever-project.org/wiki/display/docs/RO+decay+detection+using+checklists]()
* [http://www.wf4ever-project.org/wiki/display/docs/RO+checklist+evaluation+API]()
* [http://www.wf4ever-project.org/wiki/display/docs/Checklist+traffic+light+API]()

## Usability study

* Web links:
  * [https://github.com/wf4ever/ro-manager]()
  * [https://pypi.python.org/pypi/ro-manager]()
  * @@TODO need page for checklist service installation, etc.
  * [http://www.wf4ever-project.org/wiki/display/docs/RO+Manager+FAQ]()
  * [http://www.wf4ever-project.org/wiki/display/docs/Sandbox+configuration]()

### User perspective

> User perspective relates to usability by an end-user of the software

This component is not used directly by a user.  Rather, it is called by other user-facing Wf4Ever components to provide information about how well RO meets some designated criteria.  As such, it has not been subjected to a separate usability study.

@@cover visualization or ensure it is covered inother deliverables.  Cross-ref?


#### Survey

(Based on [SSI evaluation guide](http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationTutorial.pdf))

**General usability:**

* Visibility of system status. Does it give users appropriate feedback within reasonable time?
  * N/A
* Match between system and the real world. Does it speak the user’s language and make information appear in a natural and logical order? Are implementation-specific details hidden from the user?
  * N/A
* User control and freedom. Does it provide clearly marked exits, undo and redo?
  * N/A
* Consistency and standards. Is it consistent within the software, with other similar packages and with platform conventions?
  * N/A
* Error prevention. Does it prevent errors in the first place or help users avoid making them?
  * N/A
* Recognition rather than recall. Does it make objects, actions and options visible and reduce the amount of information a user has to remember?
  * N/A
* Flexibility and efficiency of use. Does it offer short-cuts and support macros for frequently- done action sequences?
  * N/A
* Aesthetic and minimalist design. Does it avoid showing irrelevant or rarely-needed information?
  * N/A
* Help users recognize, diagnose, and recover from errors. Does it make errors clear, comprehensible, precise and suggest solutions if possible?
  * N/A
* Help and documentation. Does it provide concise, accurate, clear, easily-searchable task- oriented doc centred around concrete lists of steps?
  * N/A
* Robustness.  Are responses sensible when instructions are not followed; e.g., wrong commands, illegal values, etc.?
  * N/A
* Obviousness. Can tasks be accomplished without any consultation of user documents or other material?
  * N/A

**Release packaging:**

* Are the binary releases packaged for immediate use in a suitable archive format?
  * Yes: installation via [PyPI](https://pypi.python.org/pypi/ro-manager)
  * @@TODO: checklist deployment documentation; also needs Pyramid
* Is it clear how to get the software from the web site? Does it have version numbers?
  * Yes
* Is it clear from the web site or user doc what other packages are required?
  * @@TODO - make pyramid dependency clear 
* Is it clear what the licencing and copyright is on the web site?
  * @@TODO - clarify copyright and MIT licensing
* How to get started.  Is there a README, FAQ?
  * @@TODO for checklist

**User documentation:**

* Are thererelevant user documents?
  * @@TODO (links)
* Is the user documentation accurate?
  * Moderately @@TODO: could do with review and checking
* Does it partition user, user-developer and developer information or mix it all together?
  * Sme partitioning, but not rigorous
* Is the user doc online?
  * Yes (github and Wf4Ever wiki)
* Are there any supporting tutorials?
  * No
* Do these list the versions they apply to?
  * N/A
* Is it task-oriented, structured around helping users achieve their tasks?
  * N/A

**Help and support:**

* Is there a list of known bugs and issues, or a bug/issue tracker?
  * Sort-of: [https://github.com/wf4ever/ro-manager/issues](), [https://github.com/wf4ever/ro-manager/blob/master/TODO.txt]()
* Is it clear how to ask for help e.g. where to e-mail or how to enter bugs/issues
  * No.
* Are there e-mail list archives or forums?
  * No
* If so, is there evidence of use?
  * N/A
* Are they searchable?
  * N/A
* Is there a bug/issue tracker?
  * Yes (but not much used).
  * [https://github.com/wf4ever/ro-manager/issues]()
  * [https://github.com/wf4ever/ro-manager/blob/master/TODO.txt]()
  * [https://jira.man.poznan.pl/jira/issues]() (not public)
* If so, there evidence of use?
  * Some, but not consistent
* Does it seem that bugs and issues are resolved or, at least, looked at?
  * Some
* Is it clear what quality of service a user expect in terms of support e.g. best effort, reasonable effort, reply in 24 hours etc.?
  * No
* Is it clear how to contribute bugs, issues, corrections (e.g. in tutorials or user doc) or ideas?
  * No, but use of Github offers a generic route.


### User-developer perspective

> User-developer perspective relates to usability by developer who makes use of the software in som,e opther software component.

The checklist provides a simple [REST API](http://www.wf4ever-project.org/wiki/display/docs/RO+checklist+evaluation+API) which is used by other component developers to perform a required evaluation.  There is also a ["traffic light API"](http://www.wf4ever-project.org/wiki/display/docs/Checklist+traffic+light+API) that is very similar in style.  The REST API has proved quite easy to use, and has been successfully integrated with minimal additional guidance from the checklist service developer by other components of the Wf4Ever project:

1. [Showcase 47](http://www.wf4ever-project.org/wiki/pages/viewpage.action?pageId=3506198): in this activity, the checklist service (developed by Oxford) was accessed by an early prototype quality display service (developed separately by iSOCO).  This was out first attempt to integrate Integrity and Authenticity components developed by different project members, and showed that the REST style adopted could facilitate integration of software components.
2. [myExperiment](@@ref): the checklist display has been integrated into the displayt of a myExperiment PACK.
3. [RO Portal](@@ref): the checklist display has been integrated into the RO portal display of a Research Object.

To perform a checklist evaluation, a checklist description must be created if one does not already exis. This requires some knowledge of RDF and SPARQL.  Originally, checklists were created by hand-editing RDF, which in practice meant they were initially coded by the checklist software developer.  Once an initial checklist had been created, other developers were generally able to make modest changes to the checklist (based on experiences from setting up software deminstrations, e.g. [http://www.wf4ever-project.org/wiki/display/docs/135.+TIMBUS+Demo+preparation]()).

More recently, based in part on input from a new project member, we have designed a spreadsheet based format for creating checklists, and created a tool [mkminim](https://github.com/wf4ever/ro-manager/blob/master/src/checklist/mkminim.md) for converting this to RDF for consumption by the checklist evaluation service.  At the time of writing, we have not conducted a formal usability study of this tool and the associated spreadsheet format.

As part of another approach to mitigating the possible difficulty of creating checklist descriptions, we have created an in [initial collection of example and skeleton checklists](https://github.com/wf4ever/ro-catalogue/tree/master/minim), which may be used as a starting point for creating new checklist definitions.

#### Survey

How easy is it to set up development environment to write code that uses the software or service? This may involve getting the source code of the software but for online services, it might not.

* Is it clear what third-party tools and software you need, which versions you need, where to get these and how to set them up?
  * Mostly handled by PyPI
  * @@TODO install documentation for checklist service
* Are there tutorials available for user-developers?
  * No
* If so, Do these list the versions they apply to? Are they accurate, and understandable?
  * N/A
* Is there example code that can be compiled, customised and used?
  * Yes
  * @@TODO: locate and link
* How accurate, understandable and complete is the API documentation? Does it provide
examples of use?
  * [http://www.wf4ever-project.org/wiki/display/docs/RO+checklist+evaluation+API]()
  * [http://www.wf4ever-project.org/wiki/display/docs/Checklist+traffic+light+API]()
* For services, is there information about quality of service? e.g. number of requests that can be run in a specific time period. How do user-developers find out when services might be down etc.
  * No.
* For services, is there information about copyright and licencing as to how the services can be used? e.g. for non-commercial purposes only, does the project have to be credited etc. Is there information on how any data can be used, who owns it etc.?
  * No.
<!--
* Is there a contributions policy that allows user-developers to contribute their components to the client? How restrictive is it? Who owns the contributions?
  * @@Don't understand.  What is "Client": here?
-->?
* Is the copyright and licencing of the software and third-party dependencies clear and documented so you can understand the implications on extensions you write?
  * N/A


### Developer perspective

The checklist evaluation software has been developed as part of the RO Manager software suite, and makes use of many of the same components.  Source code is written in Python, and development has followed a test-led development practice, an effect of which is that there are many examples of the code function for other develiopers for study.

Development of RO Manager has been led by a single programmer at Oxford, but part of the suite, handling exchange of Research Objects with RODL, has been written by developers at Poznan.  This gives us some confidence to claim that the code base is accessible and usable by developers who wish to enhance and/or fix the software.

@@Link to checklist service web page for developers


#### Survey

* How easy is it to set up development environment to change the software?
  * Fairly easy - standard Python environment 
  * NOTE: not tested under Windows.
  * @@TODO specific documentation needed for checklist service
* How easy is it to access to up-to-date versions of the source code that reflect changes made since the last release? i.e. access to the source code repository.
  * https://github.com/wf4ever/ro-manager
* How easy is it to understand the structure of the source code repository? Is there information that relates the structure of the source code to the software’s architecture?
  * @@TODO code structure overview in GitHub
* Is it clear what third-party tools and software you need, which versions you need, where to get these and how to set them up?
  * @@TODO install documentation
* How easy is it to compile the code?
  * Standard Python environment: no separate compilation needed.
* How easy is it to build a release bundle or deploy a service?
  * Easy (see https://github.com/wf4ever/ro-manager/blob/master/NOTES.txt)
* How easy is it to validate changes you’ve made? This includes building the software, getting, building and running tests.
  * Fairly easy
  * @@TODO install documentation
* Is there design documentation available? How accurate and understandable is it?
  * Some, not comprehensive
  * LISC paper
  * @@TODO code structure overview in GitHub
* Are there tutorials available for developers?
  * No
* If so, are they accurate, and understandable?
  * N/A
* How readable is the source code?  Well-laid out with good use of white-space and
indentation?
  * OK
* How accurate or comprehensive is the source code commenting? Does it focus on why the code is as it is?
  * Yes - comments generally explain rather than repeat code.


## Benchmarking

> This section should cover everything related to functional and non functional aspects of the component, including performance, the coverage it provides, the time necessary to execute its regular operation, etc. Whenever possible a baseline should be established against which to compare.

### Capabilities

> See [http://www.wf4ever-project.org/wiki/display/docs/Showcase+128+-+Evaluate+checklist+toolkit]()

### Performance

> @@TODO

> See [https://github.com/wf4ever/ro-catalogue/blob/master/v0.1/minim-evaluation/performance-evaluation.md]()

## Sustainability and maintainability

The checklist evaluation software has been developed as part of the RO Manager software suite, and makes use of many of the same components.  This commonality means that fixes developeed for one can benefit the other.  The total amount of dedicated checklist evaluation code is therefore quite small (see https://github.com/wf4ever/ro-manager/tree/master/src/iaeval).  The checklist service is built upon the Pyramid web application framework (http://docs.pylonsproject.org/projects/pyramid/en/latest/), but the amount of dependent code is quite small and porting to the Django framework has been considered.

The RO Manager software is packaged and distributed via PyPI [https://pypi.python.org/pypi/ro-manager](), and the standard Python installation utilities are used for installation (`pip`, or `easy_install`).  These deal with most dependencies.  One dependency of the checklist service that is not automatically installed by the RO Manager package is the Pyramid web framework, which itself can be installed with all its dependencies by a single command.

The checklist evaluation software has been designed and developed by a single programmer, with maintainability and sustainability efforts focused on technical features (unit and integration tests), and to date very little effort has been committed to developer-community building.

@@TODO - link to a README page for the checklist service


### Survey

**Identity:**

To what extent is the identity of the project/software clear and unique both within its application domain and generally?

* Project/software has its own domain name.
  * No, but it is under tghe umbrella of the Wf4Ever project which does (http://www.wf4ever-project.org)
* Project/software has a logo.
  * No, but Wf4Ever project does.
* Project/software has a distinct name within its application area. A search by Google on the name plus keywords from the application area throws up the project web site in the first page of matches.
  * Yes ("RO Mananger": 3rd and 9th hits as of 20131001.  Adding "wf4ever" provides all-relevant hits.)
* Project/software has a distinct name regardless of its application area. A search by Google on the name plus keywords from the application area throws up the project web site in the first page of matches.
  * Yes ("RO Mananger": 3rd and 9th hits as of 20131001.  Adding "wf4ever" provides all-relevant hits.)
* Project/software name does not throw up embarrassing “did you mean...” hits on Google.
  * OK (Unless "Did you mean: ROM Manager" is considered embarassing.)
* Project/software name does not violate an existing trade-mark.
  * Not obviously
* Project/software name is trade-marked.
  * No


**Copyright:**

To what extent is it clear who wrote the software and owns its copyright?

* Web site states copyright.
  * @@TODO
* Web site states who developed/develops the software, funders etc.
  * @@TODO
* If there are multiple web sites then these all state exactly the same copyright, licencing and authorship.
  * @@TODO
* Each source code file has a copyright statement.
  * No
* If supported by the language, each source code file has a copyright statement embedded within a constant.
  * No


**Licencing:**

Has an appropriate licence been adopted?

* Web site states licence.
  * @@TODO
* Software (source and binaries) has a licence.
  * @@TODO
* Software has an open source licence.
  * Yes (MIT)
* Software has an Open Software Initiative (OSI) recognised licence ([http://www.opensource.org/]()).
  * Yes (http://opensource.org/licenses/MIT)
* Each source code file has a licence header.
  * No


**Governance:**

To what extent does the project make its management, or how its software development is managed, transparent?

* Project has defined a governance policy.
  * No
* Governance policy is publicly available.
  * N/A


**Community:**

To what extent does/will an active user community exist for this product?

* Web site has statement of number of users/developers/members.
  * No
* Web site has success stories.
  * No
* Web site has quotes from satisfied users.
  * No
* Web site has list of important partners or collaborators.
  * No (but see Wf4Ever web site)
* Web site has list of the project’s publications.
  * No (but see Wf4Ever web site)
* Web site has list of third-party publications that cite the software.
  * No
* Web site has list of software that uses/bundles this software.
  * No
* Users are requested to cite the project if publishing papers based on results derived from the software.
  * No
* Users are required to cite a boilerplate citation if publishing papers based on results derived from the software.
  * No
* Users exist who are not members of the project.
  * No
* Developers exist who are not members of the project.
  * No


**Availability:**

To what extent is the software available? (The SSI evaluation uses "accessible" for this quality, but that has been changed here to avoid confusion with other uses of "accessibility")

* Binary distributions are available (whether for free, payment, registration).
  * Yes (PyPI distribution)
* Binary distributions are freely available.
  * Yes
* Binary distributions are available without the need for any registration or authorisation of access by the project.
  * Yes
* Source distributions are available (whether for free, payment, registration).
  * Yes (PyPI distribution and Github)
* Source distributions are freely available.
  * Yes
* Source distributions are available without the need for any registration or authorisation of access by the project.
  * Yes
* Access to source code repository is available (whether for free, payment, registration).
  * Yes
* Anonymous read-only access to source code repository.
  * Yes
* Ability to browse source code repository online.
  * Yes
* Repository is hosted externally to a single organisation/institution in a sustainable third- party repository (e.g. SourceForge, GoogleCode, LaunchPad, GitHub) which will live beyond the lifetime of any current funding line.
  * Yes (https://github.com/wf4ever/ro-manager)
* Downloads page shows evidence of regular releases (e.g. six monthly, bi-weekly, etc.).
  * Yes, bit not to fixed schedule (see https://pypi.python.org/pypi/ro-manager)


**Testability:**

How straightforward is it to test the software to verify modifications?

* Project has unit tests.
  * Yes
* Project has integration tests.
  * Partial
* For GUIs, project uses automated GUI test frameworks.
  * N/A
* Project has scripts for testing scenarios that have not been automated (e.g. for testing GUIs).
  * No
* Project recommends tools to check conformance to coding standards.
  * No
* Project has automated tests to check conformance to coding standards.
  * No
* Project recommends tools to check test coverage.
  * No
* Project has automated tests to check test coverage.
  * No
* A minimum test coverage level that must be met has been defined.
  * No
* There is an automated test for this minimum test coverage level.
  * N/A
* Tests are automatically run nightly.
  * No
* Continuous integration is supported – tests are automatically run whenever the source code changes.
  * No
* Test results are visible to all developers/members.
  * N/A
* Test results are visible publicly.
  * N/A
* Test results are e-mailed to a mailing list.
  * N/A
* This e-mailing list can be subscribed to by anyone.
  * N/A
* Project specifies how to set up external resources e.g. FTP servers, databases for tests.
  * No (some tests access RODL)
* Tests create their own files, database tables etc.
  * Some do


**Portability:**

To what extent can the software be used on other platforms?

* Application can be built on and run under Windows.
  * Yes in theory; not tested
* Application can be built on and run under Windows 7.
  * Yes in theory; not tested
* Application can be built on and run under Windows XP.
  * Yes in theory; not tested
* Application can be built on and run under Windows Vista.
  * Yes in theory; not tested
* Application can be built on and run under UNIX/Linux.
  * Yes
* Application can be built on and run under Solaris.
  * Not tested
* Application can be built on and run under RedHat.
  * Yes in theory; not tested
* Application can be built on and run under Debian.
  * Yes
* Application can be built on and run under Fedora.
  * Yes in theory; not tested
* Application can be built on and run under Ubuntu.
  * Yes
* Application can be built on and run under MacOSX.
  * Yes
* Browser applications run under Internet Explorer.
  * N/A
* Browser applications run under Mozilla Firefox.
  * N/A
* Browser applications run under Google Chrome.
  * N/A
* Browser applications run under Opera.
  * N/A
* Browser applications run under Safari.
  * N/A


**Supportability:**

To what extent will the product be supported currently and in the future?

* Web site has page describing how to get support.
  * No (but common Github tools apply)
* User doc has page describing how to get support.
  * No
* Software describes how to get support (in a README for command-line tools or a Help=>About window in a GUI).
  * No
* Above pages/windows/files describe, or link to, a description of “how to ask for help” e.g. cite version number, send transcript, error logs etc.
  * No
* Project has an e-mail address.
  * No (but see Wf4ever project)
* Project e-mail address has project domain name.
  * N/A
* E-mails are read by more than one person.
  * N/A
* E-mails are archived.
  * N/A
* E-mail archives are publicly readable.
  * N/A
* E-mail archives are searchable.
  * N/A
* Project has a ticketing system.
  * Yes (Github; also JIRA for Wf4Ever internal use)
* Ticketing system is publicly readable.
  * Yes
* Ticketing system is searchable.
  * Yes
* Web site has site map or index.
  * @@TODO create front page for checklist evaluation; create indexc on RO Manager page
* Web site has search facility.
  * No
* Project resources are hosted externally to a single organisation/institution in a sustainbable e-mail   archives or ticketing system shows that queries are responded to within a week (not necessarily fixed, but at least looked at and a decision taken as to their priority).
  * Source code, web pages and developer facilities are hosted in GitHub; some supporting material is on Wf4Ever project servers.
  * See also Wf4ever project
* If there is a blog, is it is regularly used.
  * N/A
  * See also Wf4ever project
* E-mail lists or forums, if present, have regular posts.
  * N/A


**Analysability:**

How straightforward is it to analyse the software’s source release to:
(a) To understand its implementation architecture?
(b) To understand individual source code files and how they fit into the implementation architecture?

* Source code is structured into modules or packages.
  * Yes
* Source code structure relates clearly to the architecture or design.
  * Yes, but could be improved
* Project files for IDEs are provided.
  * N/A
* Source code repository is a revision control system.
  * Yes
* Structure of the source code repository and how this maps to the software’s components is documented.
  * @@TODO code structure overview in GitHub
* Source releases are snapshots of the repository.
  * Yes: PyPI releases are created from commits on the repository master branch.
* Source code is commented.
  * Yes
* Source code comments are written in an API document generation mark-up language e.g. JavaDoc or Doxygen.
  * N/A
* Source code is laid out and indented well.
  * Yes
* Source code uses sensible class, package and variable names.
  * Yes
* There are no old source code files that should be handled by version control e.g. “SomeComponentOld.java”.
  * Usually not.  Some are kept transiently, and removed after changes are finalized.
* There is no commented out code.
  * Some, but generally only kept transiently.
* There are no TODOs in the code.
  * Lots of TODOs
* Coding standards are required to be observed.
  * No enforcement
* Project-specific coding standards are consistent with community or generic coding standards (e.g. for C, Java, FORTRAN etc.).
  * Yes


**Changeability:**

How straightforward is it to modify the software to:
(a) Address issues?
(b) Modify functionality?
(c) Add new functionality?


* Project has defined a contributions policy.
  * No
* Contributions policy is publicly available.
  * No
* Contributors retain copyright/IP of their contributions.
  * Undetermined
* Users, user-developers and developers who are not project members can contribute.
  * Yes, through gatekeeper
* Project has defined a stability/deprecation policy for components, APIs etc.
  * No
* Stability/deprecation policy is publicly available.
  * N/A
* Releases document deprecated components/APIs in that release.
  * N/A (no components or APIs have been deprecated)
* Releases document removed/changed components/APIs in that release.
  * N/A (no components or APIs have been removed/changed)
* Changes in the source code repository are e-mailed to a mailing list.
  * No
* This e-mailing list can be subscribed to by anyone.
  * N/A


**Evolvability:**

To what extent will the product be developed in the future:
(a) For a future release?
(b) Within a roadmap for the product?
￼￼

* Web site describes project roadmap or plans or milestones (either on a web page or within a ticketing system).
  * No
* Web site describes how project is funded/sustained.
  * No
  * See also Wf4ever project
* We site describes end dates of current funding lines.
  * No
  * See also Wf4ever project


**Interoperability:**

To what extent does the software’s interoperability:
(a) Meet appropriate open standards?
(b) Function with required third-party components?
(c) Function with optional third-party components?

* Uses open standards.
  * Yes (HTTP, RDF, etc.)
* Uses mature, ratified, non-draft open standards.
  * Yes
* Provides tests demonstrating compliance to open standards.
  * No


## Summary of evaluation for checklist service

The checklist evaluation service is targeted at developers who wish to use the checklist service to provide quality indications of Research Objects in other components of workflow preservation software.  Our experiences indicate that the API is easy to use, and the code base is capable of being understood and maintained by third party developers.

The checklist creation aspects of this work need further work to make them accessible to end users; we have made some initial steps towards this goal.  The ease with which checklists have been adapted indicates that the existing tools are somewhat usable by developers with some knowledge of RDF. SPARQL and the RO Model.

The project has many of the technical requirements for sustainability and maintainability inplace (version control, unit tests, commented source code, etc.).  The main missing technical feature is to use a continuous integration platform.  But it is clear from the SSI checklist that there are many aspects of developer community building and communication that are not yet in place.  These are points that should be addressed if a future project attempts to move this service from a research prototype to a fully fledged software product.




<!--
    =================================================================================================
-->



# Stability service

@@TODO: Esteban this section?


## Component description

@@brief description

See also:
* (D4.2)
* (other links)


## Usability study

@@links to supporting documentation??


### User perspective

@@description of end-user facing usability

@@cover visualization or ensure it is covered inother deliverables.  Cross-ref?


#### Survey

@@TODO: as appropriate, replace "N/A" or TODO by appropriate survey responses

**General usability:**

* Visibility of system status. Does it give users appropriate feedback within reasonable time?
  * N/A
* Match between system and the real world. Does it speak the user’s language and make information appear in a natural and logical order? Are implementation-specific details hidden from the user?
  * N/A
* User control and freedom. Does it provide clearly marked exits, undo and redo?
  * N/A
* Consistency and standards. Is it consistent within the software, with other similar packages and with platform conventions?
  * N/A
* Error prevention. Does it prevent errors in the first place or help users avoid making them?
  * N/A
* Recognition rather than recall. Does it make objects, actions and options visible and reduce the amount of information a user has to remember?
  * N/A
* Flexibility and efficiency of use. Does it offer short-cuts and support macros for frequently- done action sequences?
  * N/A
* Aesthetic and minimalist design. Does it avoid showing irrelevant or rarely-needed information?
  * N/A
* Help users recognize, diagnose, and recover from errors. Does it make errors clear, comprehensible, precise and suggest solutions if possible?
  * N/A
* Help and documentation. Does it provide concise, accurate, clear, easily-searchable task- oriented doc centred around concrete lists of steps?
  * N/A
* Robustness.  Are responses sensible when instructions are not followed; e.g., wrong commands, illegal values, etc.?
  * N/A
* Obviousness. Can tasks be accomplished without any consultation of user documents or other material?
  * N/A

**Release packaging:**

* Are the binary releases packaged for immediate use in a suitable archive format?
  * @@TODO
* Is it clear how to get the software from the web site? Does it have version numbers?
  * @@TODO
* Is it clear from the web site or user doc what other packages are required?
  * @@TODO
* Is it clear what the licencing and copyright is on the web site?
  * @@TODO
* How to get started.  Is there a README, FAQ?
  * @@TODO

**User documentation:**

* Are thererelevant user documents?
  * @@TODO (links)
* Is the user documentation accurate?
  * @@TODO
* Does it partition user, user-developer and developer information or mix it all together?
  * @@TODO
* Is the user doc online?
  * @@TODO
* Are there any supporting tutorials?
  * @@TODO
* If so, do these list the versions they apply to?
  * N/A
* If so, are they task-oriented, structured around helping users achieve their tasks?
  * N/A

**Help and support:**

* Is there a list of known bugs and issues, or a bug/issue tracker?
  * @@TODO
* Is it clear how to ask for help e.g. where to e-mail or how to enter bugs/issues
  * @@TODO
* Are there e-mail list archives or forums?
  * @@TODO
* If so, is there evidence of use?
  * N/A
* Are they searchable?
  * N/A
* Is there a bug/issue tracker?
  * @@TODO
* If so, there evidence of use?
  * N/A
* If so, does it seem that bugs and issues are resolved or, at least, looked at?
  * N/A
* Is it clear what quality of service a user expect in terms of support e.g. best effort, reasonable effort, reply in 24 hours etc.?
  * @@TODO
* Is it clear how to contribute bugs, issues, corrections (e.g. in tutorials or user doc) or ideas?
  * @@TODO


### User-developer perspective

@@TODO: describe user-developer capabilities and interface

#### Survey

* How easy is it to set up development environment to write code that uses the software or service?
  * @@TODO
* Is it clear what third-party tools and software you need, which versions you need, where to get these and how to set them up?
  * @@TODO
* Are there tutorials available for user-developers?
  * @@TODO
* If so, Do these list the versions they apply to? Are they accurate, and understandable?
  * N/A
* Is there example code that can be compiled, customised and used?
  * @@TODO
* How accurate, understandable and complete is the API documentation? Does it provide
examples of use?
  * @@TODO
* For services, is there information about quality of service? e.g. number of requests that can be run in a specific time period. How do user-developers find out when services might be down etc.
  * @@TODO
* For services, is there information about copyright and licencing as to how the services can be used? e.g. for non-commercial purposes only, does the project have to be credited etc. Is there information on how any data can be used, who owns it etc.?
  * @@TODO
* Is the copyright and licencing of the software and third-party dependencies clear and documented so you can understand the implications on extensions you write?
  * @@TODO


### Developer perspective

@@Briefly describe the nature of the codebase, language used and development practices

@@Link to component web page for developers


#### Survey

* How easy is it to set up development environment to change the software?
  * @@TODO
* How easy is it to access to up-to-date versions of the source code that reflect changes made since the last release? i.e. access to the source code repository.
  * @@TODO
* How easy is it to understand the structure of the source code repository? Is there information that relates the structure of the source code to the software’s architecture?
  * @@TODO
* Is it clear what third-party tools and software you need, which versions you need, where to get these and how to set them up?
  * @@TODO
* How easy is it to compile the code?
  * @@TODO
* How easy is it to build a release bundle or deploy a service?
  * @@TODO
* How easy is it to validate changes you’ve made? This includes building the software, getting, building and running tests.
  * @@TODO
* Is there design documentation available? How accurate and understandable is it?
  * @@TODO
* Are there tutorials available for developers?
  * @@TODO
* If so, are they accurate, and understandable?
  * @@TODO
* How readable is the source code?  Well-laid out with good use of white-space and
indentation?
  * @@TODO
* How accurate or comprehensive is the source code commenting?  Does it focus on why the code is as it is?
  * @@TODO


## Benchmarking

### Capabilities

@@TODO - describe evaluation and result


### Performance

@@TODO - describe evaluation and result


## Sustainability and maintainability

@@TODO: brief background information from developer perspective, focusing on access, support and communication

### Survey

**Identity:**

To what extent is the identity of the project/software clear and unique both within its application domain and generally?

* Project/software has its own domain name.
  * @@TODO
* Project/software has a logo.
  * @@TODO
* Project/software has a distinct name within its application area. A search by Google on the name plus keywords from the application area throws up the project web site in the first page of matches.
  * @@TODO
* Project/software has a distinct name regardless of its application area. A search by Google on the name plus keywords from the application area throws up the project web site in the first page of matches.
  * @@TODO
* Project/software name does not throw up embarrassing “did you mean...” hits on Google.
  * @@TODO
* Project/software name does not violate an existing trade-mark.
  * @@TODO
* Project/software name is trade-marked.
  * @@TODO


**Copyright:**

To what extent is it clear who wrote the software and owns its copyright?

* Web site states copyright.
  * @@TODO
* Web site states who developed/develops the software, funders etc.
  * @@TODO
* If there are multiple web sites then these all state exactly the same copyright, licencing and authorship.
  * @@TODO
* Each source code file has a copyright statement.
  * @@TODO
* If supported by the language, each source code file has a copyright statement embedded within a constant.
  * @@TODO


**Licencing:**

Has an appropriate licence been adopted?

* Web site states licence.
  * @@TODO
* Software (source and binaries) has a licence.
  * @@TODO
* Software has an open source licence.
  * @@TODO
* Software has an Open Software Initiative (OSI) recognised licence ([http://www.opensource.org/]()).
  * @@TODO
* Each source code file has a licence header.
  * @@TODO


**Governance:**

To what extent does the project make its management, or how its software development is managed, transparent?

* Project has defined a governance policy.
  * @@TODO
* Governance policy is publicly available.
  * @@TODO


**Community:**

To what extent does/will an active user community exist for this product?

* Web site has statement of number of users/developers/members.
  * @@TODO
* Web site has success stories.
  * @@TODO
* Web site has quotes from satisfied users.
  * @@TODO
* Web site has list of important partners or collaborators.
  * @@TODO
* Web site has list of the project’s publications.
  * @@TODO
* Web site has list of third-party publications that cite the software.
  * @@TODO
* Web site has list of software that uses/bundles this software.
  * @@TODO
* Users are requested to cite the project if publishing papers based on results derived from the software.
  * @@TODO
* Users are required to cite a boilerplate citation if publishing papers based on results derived from the software.
  * @@TODO
* Users exist who are not members of the project.
  * @@TODO
* Developers exist who are not members of the project.
  * @@TODO


**Availability:**

To what extent is the software available? (The SSI evaluation uses "accessible" for this quality, but that has been changed here to avoid confusion with other uses of "accessibility")

* Binary distributions are available (whether for free, payment, registration).
  * @@TODO
* Binary distributions are freely available.
  * @@TODO
* Binary distributions are available without the need for any registration or authorisation of access by the project.
  * @@TODO
* Source distributions are available (whether for free, payment, registration).
  * @@TODO
* Source distributions are freely available.
  * @@TODO
* Source distributions are available without the need for any registration or authorisation of access by the project.
  * @@TODO
* Access to source code repository is available (whether for free, payment, registration).
  * @@TODO
* Anonymous read-only access to source code repository.
  * @@TODO
* Ability to browse source code repository online.
  * @@TODO
* Repository is hosted externally to a single organisation/institution in a sustainable third- party repository (e.g. SourceForge, GoogleCode, LaunchPad, GitHub) which will live beyond the lifetime of any current funding line.
  * @@TODO
* Downloads page shows evidence of regular releases (e.g. six monthly, bi-weekly, etc.).
  * @@TODO


**Testability:**

How straightforward is it to test the software to verify modifications?

* Project has unit tests.
  * @@TODO
* Project has integration tests.
  * @@TODO
* For GUIs, project uses automated GUI test frameworks.
  * @@TODO
* Project has scripts for testing scenarios that have not been automated (e.g. for testing GUIs).
  * @@TODO
* Project recommends tools to check conformance to coding standards.
  * @@TODO
* Project has automated tests to check conformance to coding standards.
  * @@TODO
* Project recommends tools to check test coverage.
  * @@TODO
* Project has automated tests to check test coverage.
  * @@TODO
* A minimum test coverage level that must be met has been defined.
  * @@TODO
* There is an automated test for this minimum test coverage level.
  * @@TODO
* Tests are automatically run nightly.
  * @@TODO
* Continuous integration is supported – tests are automatically run whenever the source code changes.
  * @@TODO
* Test results are visible to all developers/members.
  * @@TODO
* Test results are visible publicly.
  * @@TODO
* Test results are e-mailed to a mailing list.
  * @@TODO
* This e-mailing list can be subscribed to by anyone.
  * @@TODO
* Project specifies how to set up external resources e.g. FTP servers, databases for tests.
  * @@TODO
* Tests create their own files, database tables etc.
  * @@TODO


**Portability:**

To what extent can the software be used on other platforms?

* Application can be built on and run under Windows.
  * @@TODO
* Application can be built on and run under Windows 7.
  * @@TODO
* Application can be built on and run under Windows XP.
  * @@TODO
* Application can be built on and run under Windows Vista.
  * @@TODO
* Application can be built on and run under UNIX/Linux.
  * @@TODO
* Application can be built on and run under Solaris.
  * @@TODO
* Application can be built on and run under RedHat.
  * @@TODO
* Application can be built on and run under Debian.
  * @@TODO
* Application can be built on and run under Fedora.
  * @@TODO
* Application can be built on and run under Ubuntu.
  * @@TODO
* Application can be built on and run under MacOSX.
  * @@TODO
* Browser applications run under Internet Explorer.
  * @@TODO
* Browser applications run under Mozilla Firefox.
  * @@TODO
* Browser applications run under Google Chrome.
  * @@TODO
* Browser applications run under Opera.
  * @@TODO
* Browser applications run under Safari.
  * @@TODO


**Supportability:**

To what extent will the product be supported currently and in the future?

* Web site has page describing how to get support.
  * @@TODO
* User doc has page describing how to get support.
  * @@TODO
* Software describes how to get support (in a README for command-line tools or a Help=>About window in a GUI).
  * @@TODO
* Above pages/windows/files describe, or link to, a description of “how to ask for help” e.g. cite version number, send transcript, error logs etc.
  * @@TODO
* Project has an e-mail address.
  * @@TODO
* Project e-mail address has project domain name.
  * @@TODO
* E-mails are read by more than one person.
  * @@TODO
* E-mails are archived.
  * @@TODO
* E-mail archives are publicly readable.
  * @@TODO
* E-mail archives are searchable.
  * @@TODO
* Project has a ticketing system.
  * @@TODO
* Ticketing system is publicly readable.
  * @@TODO
* Ticketing system is searchable.
  * @@TODO
* Web site has site map or index.
  * @@TODO
* Web site has search facility.
  * @@TODO
* Project resources are hosted externally to a single organisation/institution in a sustainbable e-mail   archives or ticketing system shows that queries are responded to within a week (not necessarily fixed, but at least looked at and a decision taken as to their priority).
  * @@TODO
* If there is a blog, is it is regularly used.
  * @@TODO
* E-mail lists or forums, if present, have regular posts.
  * @@TODO


**Analysability:**

How straightforward is it to analyse the software’s source release to:
(a) To understand its implementation architecture?
(b) To understand individual source code files and how they fit into the implementation architecture?


* Source code is structured into modules or packages.
  * @@TODO
* Source code structure relates clearly to the architecture or design.
  * @@TODO
* Project files for IDEs are provided.
  * @@TODO
* Source code repository is a revision control system.
  * @@TODO
* Structure of the source code repository and how this maps to the software’s components is documented.
  * @@TODO
* Source releases are snapshots of the repository.
  * @@TODO
* Source code is commented.
  * @@TODO
* Source code comments are written in an API document generation mark-up language e.g. JavaDoc or Doxygen.
  * @@TODO
* Source code is laid out and indented well.
  * @@TODO
* Source code uses sensible class, package and variable names.
  * @@TODO
* There are no old source code files that should be handled by version control e.g. “SomeComponentOld.java”.
  * @@TODO
* There is no commented out code.
  * @@TODO
* There are no TODOs in the code.
  * @@TODO
* Coding standards are required to be observed.
  * @@TODO
* Project-specific coding standards are consistent with community or generic coding standards (e.g. for C, Java, FORTRAN etc.).
  * @@TODO


**Changeability:**

How straightforward is it to modify the software to:
(a) Address issues?
(b) Modify functionality?
(c) Add new functionality?


* Project has defined a contributions policy.
  * @@TODO
* Contributions policy is publicly available.
  * @@TODO
* Contributors retain copyright/IP of their contributions.
  * @@TODO
* Users, user-developers and developers who are not project members can contribute.
  * @@TODO
* Project has defined a stability/deprecation policy for components, APIs etc.
  * @@TODO
* Stability/deprecation policy is publicly available.
  * @@TODO
* Releases document deprecated components/APIs in that release.
  * @@TODO
* Releases document removed/changed components/APIs in that release.
  * @@TODO
* Changes in the source code repository are e-mailed to a mailing list.
  * @@TODO
* This e-mailing list can be subscribed to by anyone.
  * @@TODO


**Evolvability:**

To what extent will the product be developed in the future:
(a) For a future release?
(b) Within a roadmap for the product?
￼￼

* Web site describes project roadmap or plans or milestones (either on a web page or within a ticketing system).
  * @@TODO
* Web site describes how project is funded/sustained.
  * @@TODO
* We site describes end dates of current funding lines.
  * @@TODO


**Interoperability:**

To what extent does the software’s interoperability:
(a) Meet appropriate open standards?
(b) Function with required third-party components?
(c) Function with optional third-party components?

* Uses open standards.
  * @@TODO
* Uses mature, ratified, non-draft open standards.
  * @@TODO
* Provides tests demonstrating compliance to open standards.
  * @@TODO


## Summary of evaluation for stability service

@@TODO: summary of reports and survey indications for component



# Conclusions

@@TODO: Jose to draft something?


# References

[[SSI-guide]] _Software Evaluation Guide_, Software Sustainability Institute, November 2011; [http://www.software.ac.uk/software-evaluation-guide][SSI_guide].

[[SSI-tutorial]] _Software Evaluation: Tutorial-based Assessment_, Software Sustainability Institute, November 2011;  [http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationTutorial.pdf][SSI-tutorial].

[[SSI-criteria]] _Software Evaluation: Criteria-based Assessment_, Software Sustainability Institute, November 2011;  [http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationCriteria.pdf][SSI-criteria]

