# Stability and Reliability service

@@TODO: Esteban this section?

The above introduced completeness of a research object provides information of the degree by which a research object contains all the required resources necessary for a purpose (e.g., workflow runnability). Based on this dimension the stability measures the ability of a workflow to preserve its overall completeness state throughout a given period of time. Thereby, stability extends the scope of the analysis from a particular point in time to a given time period. Parameters like the impact of the information added or removed from the research object and of the decay suffered throughout its history are taken into account for its assessment. 

We also have defined during the Y3 of the project another quality dimension so called reliability which measures the confidence that a scientist can have in a particular workflow to preserve its capability to be executed correctly and producing the expected results. A reliable workflow is expected not only to be free of decay at the moment of being inspected but also throughout its life. Consequently, in order to establish the reliability of a workflow we have identified to what extent it is complete with respect to a number of requirements and how stable it has been with respect to such requirements historically.

Using these two dimensions we have created an analytic tool (RO monitoring)^[http://sandbox.wf4ever-project.org/decayMonitoring/visual.html?RO=ro_uri] that enables scientists and other stakeholders to visualize these metrics and have a better understanding of the evolution of workflow reliability over time. This analytic tool also monitors the current stored ROs providing a notification service which alerts whenever a RO suffers of decay. 

The formal definition of these dimensions and their scores calculation can be found at D4.2v2 “Design, implementation and deployment of workflow integrity and authenticity maintenance components” \cite{D4.2v2}, and also at \cite{wf_reliability} \cite{wf_history}.


## Component description

The reliability of a workflow measures the ability of a RO for converging towards a scenario free of decay, i.e. complete and stable through time. A reliable workflow is expected not only to be free of decay at the moment of being inspected but also in general throughout its life span. Consequently, in order to establish the reliability of a workflow it becomes necessary to assess to what extent it is complete with respect to a number of requirements and how stable it has been with respect to such requirements historically. For that purpose this component makes used of the different scores for the checklist and stability assessments and then it calculates the reliability of a RO.  

The different scores for the completeness dimension are provided by the checklist evaluation API^[http://www.wf4ever-project.org/wiki/display/docs/RO+checklist+evaluation+API]^[http://www.wf4ever-project.org/wiki/display/docs/Reliability+Evaluation+API], and the stability evaluation is subsumed into the reliability API  which provides results for both dimensions simultaneously (see \cite{D4.2v2} for further details). 


## Usability study

Links:
Definition of Stability: http://www.wf4ever-project.org/wiki/display/docs/Definition+of+RO+Stability
Reliability API: http://www.wf4ever-project.org/wiki/display/docs/Reliability+Evaluation+API
Reliability Notifications: http://www.wf4ever-project.org/wiki/display/docs/Reliability+Notifications
Visual RO Monitoring: http://sandbox.wf4ever-project.org/decayMonitoring/visual.html?RO=ro_uri



### User perspective

The main usage of the Reliability and Stability services is by using the RO Monitoring webapp. 
To see the monitoring of a RO users have a link in each of the ROs available in the RO Portal. 
They will have access to the complete trace of reliability in an easy an visual way. 
Apart from that, users will be notified by the notifications system by any variation in terms of quality of the RO.
Once the RO monitoring tool is opened, a graph containing quality information over time will be displayed. 
Users may have the opportunity to click on specific dates and get the checklist evaluation of that day, including a description and de possibility to compare it against a different point in time. 
As we can see in the following image, the points in the graph are in three different color (red, yellow, green) depending on the checklist results.
The descriptions of the checklist appear under the graph once the user has clicked on a date.
The option to compare two different dates or to analyze one day only is selected at the Combobox placed on top of the screen.

![alt tag](http://www.wf4ever-project.org/wiki/download/attachments/4459476/qualityReport2.PNG)


#### Survey

##### General usability:

* Visibility of system status. Does it give users appropriate feedback within reasonable time?
    * Yes. The results appear in a negligible amount of time, providing the needed feedback to the user.
* Match between system and the real world. Does it speak the user’s language and make information appear in a natural and logical order? Are implementation-specific details hidden from the user?
    * Yes. The usage of graphs makes the information understandable. Some concepts may require contextual knowledge but are presented in user's language.
* User control and freedom. Does it provide clearly marked exits, undo and redo?
    * Yes. The undo and redo are not needed in this application, but navigation is very intuitive.
* Consistency and standards. Is it consistent within the software, with other similar packages and with platform conventions?
    * Yes. The languange, interaction with other applications and style are aligned with the project standards.
* Error prevention. Does it prevent errors in the first place or help users avoid making them?
    * Yes. The application presents a set of information that doesn't allow users make errors. In case something could happen a message would appear in order to help users.
* Recognition rather than recall. Does it make objects, actions and options visible and reduce the amount of information a user has to remember?
    * Yes. The display and interface is clean and simple.
* Flexibility and efficiency of use. Does it offer short-cuts and support macros for frequently- done action sequences?
    * No. They are not needed.
* Aesthetic and minimalist design. Does it avoid showing irrelevant or rarely-needed information?
    * Yes. Only the needed information is visible.
* Help users recognize, diagnose, and recover from errors. Does it make errors clear, comprehensible, precise and suggest solutions if possible?
    * Yes. Errors are clear, but as it is an informative application the recoverability is not needed.
* Help and documentation. Does it provide concise, accurate, clear, easily-searchable task- oriented doc centred around concrete lists of steps?
    * Yes. There is information on the project main webpage and project wiki.
* Robustness.  Are responses sensible when instructions are not followed; e.g., wrong commands, illegal values, etc.?
    * Yes. The only possible of error is the retrieval of an inexistent Research Object, and a message would appear to inform about that.
* Obviousness. Can tasks be accomplished without any consultation of user documents or other material?
    * Yes. Everything can be used, but a introduction to the concepts would be necessary to understand the meaning of the presented results.

##### Release packaging:

* Are the binary releases packaged for immediate use in a suitable archive format?
    * N/A. Since it is a webservice with a web application, users do not need a binary release.
* Is it clear how to get the software from the web site? Does it have version numbers?
    * Yes, in case it is needed there is access to github to download the release, but version numbers are not used.
* Is it clear from the web site or user doc what other packages are required?
    * Yes, and they are included in the project.
* Is it clear what the licencing and copyright is on the web site?
    * Yes, it appears on the site.
* How to get started.  Is there a README, FAQ?
    * Yes, there is information at the project page.

##### User documentation:

* Are thererelevant user documents?
    * Yes, there are documents in the project page and wiki. Some of them have been provided at the begining of this section.
* Is the user documentation accurate?
    * Yes, it provides enough information to use the web application.
* Does it partition user, user-developer and developer information or mix it all together?
    * Yes, the information is separated.
* Is the user doc online?
    * Yes.
* Are there any supporting tutorials?
    * No, but everything is described in the documentation.
* If so, do these list the versions they apply to?
    * Not needed.
* If so, are they task-oriented, structured around helping users achieve their tasks?
    * Not needed.

##### Help and support:

* Is there a list of known bugs and issues, or a bug/issue tracker?
    * No. There is no list of bugs.
* Is it clear how to ask for help e.g. where to e-mail or how to enter bugs/issues
    * Yes. From github one can find the developer's e-mails.
* Are there e-mail list archives or forums?
    * No.
* If so, is there evidence of use?
    * Not available.
* Are they searchable?
    * Not available.
* Is there a bug/issue tracker?
    * Yes, a private one.
* If so, there evidence of use?
    * Yes, but not public.
* If so, does it seem that bugs and issues are resolved or, at least, looked at?
    * Depending on the bug.
* Is it clear what quality of service a user expect in terms of support e.g. best effort, reasonable effort, reply in 24 hours etc.?
    * No, there is no information in respect to that.
* Is it clear how to contribute bugs, issues, corrections (e.g. in tutorials or user doc) or ideas?
    * There is the possibility of using Github as a generic way.


### User-developer perspective

The user-developers will find that the project is very easy to set up. Libraries and references are included in the project and as long as external services work, the Stability and Reliability services will also work. The user-developers will need a Server to deploy the project and also an Eclipse IDE in case they want to change part of the functionality and/or customize it.
#### Survey

* How easy is it to set up development environment to write code that uses the software or service?
    * Easy, download the java project (developed in Eclipse IDE) and run it on a Tomcat.
* Is it clear what third-party tools and software you need, which versions you need, where to get these and how to set them up?
    * Yes, they are included in the project.
* Are there tutorials available for user-developers?
    * Not specifically for developers, but the user documents may help.
* If so, Do these list the versions they apply to? Are they accurate, and understandable?
    * Not available.
* Is there example code that can be compiled, customised and used?
    * The whole project can be compiled and used.
* How accurate, understandable and complete is the API documentation? Does it provide
examples of use?
    * Yes, the API is well described and available.
* For services, is there information about quality of service? e.g. number of requests that can be run in a specific time period. How do user-developers find out when services might be down etc.
    * No, this information is not available.
* For services, is there information about copyright and licencing as to how the services can be used? e.g. for non-commercial purposes only, does the project have to be credited etc. Is there information on how any data can be used, who owns it etc.?
    * @@TODO
* Is the copyright and licencing of the software and third-party dependencies clear and documented so you can understand the implications on extensions you write?
    * @@TODO


### Developer perspective

The codebase of the application is written in Java, and the API is based on the use of RESTfull web services in the same line of other services developed for the project. The web applications uses javascript and jQuery. The code is available at Github and follows the recommended java best practices.

@@Link to component web page for developers


#### Survey

* How easy is it to set up development environment to change the software?
    * It is easy because everything except from the server and eclipse is included in the java project.
* How easy is it to access to up-to-date versions of the source code that reflect changes made since the last release? i.e. access to the source code repository.
    * Easy, once they are uploaded to github.
* How easy is it to understand the structure of the source code repository? Is there information that relates the structure of the source code to the software’s architecture?
    * Mediium, it is structured but not described.
* Is it clear what third-party tools and software you need, which versions you need, where to get these and how to set them up?
    * Yes, the information is available in the project.
* How easy is it to compile the code?
    * Easy, not more difficult than any other code.
* How easy is it to build a release bundle or deploy a service?
    * Easy, nothing different from a standard service.
* How easy is it to validate changes you’ve made? This includes building the software, getting, building and running tests.
    * Easy, after the compilation everything can be proved, but there are no tests available.
* Is there design documentation available? How accurate and understandable is it?
    * A bit of high level design documentation, but not so much apart from commented code and general information at the wiki page.
* Are there tutorials available for developers?
    * No tutorials for developers.
* If so, are they accurate, and understandable?
    * Not available.
* How readable is the source code?  Well-laid out with good use of white-space and
indentation?
    * Readable, indented and with space.
* How accurate or comprehensive is the source code commenting?  Does it focus on why the code is as it is?
    * It focuses on adding information for the methods and functions and how do they work.


## Benchmarking and performance

The reliability measurement and the implemented visual analytic tool have the main goal of helping end-users (e.g. scientist in the astrophysics and bioinformatics domain) by providing them with a set of indicators which would allow a better judgment whether they want to use one RO or another based on how reliable they are. This criterion has been implemented providing a set of new indicators which have been tested and validated by end users and by usability criteria. 

The scenario for validation is the reuse of ROs by scientists. In this scenario a scientist (Bob) has a list of several tens of galaxies he has observed during the last years. He is trying to find a workflow that queries the services of the International Virtual Observatory4 (VO) in order to gather additional physical properties for his galaxies. Related to the tag extragalactic, Bob finds a promising workflow in a research object published by Alice. He reads its description and finds some similarities to his problem. He also has a list of galaxies and would like to query several web services to access their physical properties and perform similar calculations on them. Bob inspects the research object and, after successfully running the workflow, finally feels confident that Alice’s workflow is a perfect candidate for reuse in his own work. However, a deeper analysis of its recent history could prove otherwise: 

1.  The workflow evolution history shows that one of the web services changed the format of the input data when adopting ObsTAP VO5 standards for multidata querying. As a consequence the workflow broke, and authors had to replace the format of the input dataset.
2.  This dataset was also used in a script for calculating derived properties. The modification of the format of the dataset had consequences in the script, which also had to be updated. Bob thinks this may be very easily prone to errors.
3.  Later on, another web service became unavailable during a certain time, which turned out that the service provider (in fact Bob’s research institution) forgot to renew the domain and the service was down during two days. The same happened to the input data, since they were hosted in the same institution. Bob would prefer now to use his own input dataset, and not to rely on these ones. 
4.  This was not the only time the workflow experienced decay due to problems with its web services. Recent replacement of networking infrastructure (optic fiber and routing hardware) had caused connectivity glitches in the same institution, which is the provider of the web service and input datasets. Bob needs his workflow working regularly, since it continuously looks for upgraded data for his statistical study. 
5.  Finally, very recently a data provider modified the output format of the responses from HTML to VOTable6 format in order to be VO compliant and achieve data interoperability. This caused one of the scripts to fail and required the authors to fix it in order to deal with VOTable format instead of proprietary HTML format. Bob thinks this is another potential cause for having scripts behaving differently and not providing good results.

Even though the workflow currently seems to work well, Bob does not feel confident about it. The analysis shows that trustworthy reuse by scientists like Bob depends not only on the degree to which the properties of a particular workflow and its corresponding research object are preserved but also on their history. Workflows which can be executed at a particular point in time might decay and become unrunnable in the future if they depend on brittle service or data infrastructure, especially when these belong to third party institutions. Likewise, if they are subject to frequent changes by their author and contributors, the probability that some error is introduced also increases. 

...............


Due to collecting the necessary data for evaluating the above introduced scenario using the implemented tools in a real-life setting will require several years after deployment in a production environment (e.g. myExperiment) we have created a scenario which simulates a real one based on empirical data obtained as result of the study done during the Y2 of the project [3]. In that work it was studied the different types of decay in Taverna workflows and obtained real data about the distribution of decay during a period of four years. It was also shown that the most recent workflows are less prone to failures than the older ones, the main explanation being that workflows seem to be no longer maintained after some time since their creation. This makes them less reusable in time, e.g. the amount of workflows created in 2007 suffering from decay was 91% whereas in the case of more recent workflows (2011) it was around 50%. 

We have used this empirical data for characterizing how workflows decay along the time and using a sample of 100 workflows during a year we have identified three main initial groups of workflows: i) G1 contains the workflow samples which actually run and are well maintained by their creator or any other user which has a curator role. G1 workflows are less prone to decay that any other workflow in the other groups; ii) G2 contains those workflows which currently run but are not well maintained by its creator or by a curator. As a consequence G2 workflows can suffer from unexpected decay, especially in the event of changes in external resources necessary for execution; iii) G3 workflows currently do not work properly and there is no guarantee that they will be curated at some point. 

In order to model the evolution in time of our workflow population it has been considered two different states: an initial state S1 at the current time and a final state S2 at the end of the sampling period. The distribution of samples considered for each state was obtained from the study [3]. Table 1 shows the percentage of decayed workflow for each year, indicating a ratio of decay in the end of the fourth year of 41%. We have used this information to establish the initial and final states of the simulation: the initial state contains 50% workflows that work correctly (according to the data taken from 2011) whereas the final state contains only 9% of the workflows that do so (2007). The distribution of G1, G2 and G3 workflows in the initial and final states of the sample of 100 individuals are (40, 93), (20,0) and (40, 7) for each group, respectively.

Year        2007    2008    2009    2010    2011
-----      -----   -----   -----   -----   -----
Failure %     91      80      90      50      50

Table: 1 Percentage of workflows suffering decay per year.

Given that the initial state converges towards the final state by a constant day probability Pd, meaning the likeliness that a workflow changes to another group, we have defined three parameters: Pd(G1) ∝ (1- Stability) which establishes the probability that a workflow in G1 (good health) is downgraded to G3 (bad health), Pd(G2) which follows a random distribution for establishing the probability that a workflow in G2 shifts to G1 or G3, and Pd(G3) ∝ Stability which establishes the probability that a workflow in G3 is upgraded to G1. For practical reasons we have subsumed G2 into G1 and G3 (thus having two groups representing a bad and good behavior), although preserving its individual random behavior. Note that decay tends to increase as we approach the final state S2, hence increasing the population of G3 as shown in Fig. 1. The probabilities that a change occurs in a specific day (Pd) also follow the analysis results of [3]. Hereby, we have defined Pd(G1) = 0.49 and Pd(G3) = 0.38, meaning in practice that a workflow will experience three changes of group on average during the year. 

@@TODO Figure 1 Temporal evolution of the two groups (good and bad behavior)

This simulated scenario was implemented following the above explained model and its pseudocode is shown at Figure 2 where lines 6 and 10 rank the different workflows of each group proportionally to their stability values (1 -stability for G3); then lines 7 and 11 take one of them from the 20% first ranked workflows. This ranking method reflects the fact that well maintained workflows will hardly be downgraded from G1 and the opposite for G3 workflows.
 
@@TODO Figure 2 Algorithm for simulating workflows’ behavior evolution

The evaluation of the scenario is done by measuring the potential benefit for a successful reuse taking into account a historical perspective on the health of scientific workflows, represented by the reliability score, as opposed to instantaneous quality measures like the completeness value. To this purpose we run an experiment with nine scientists (as end users) from the Astrophysics domain^[http://www.iaa.es]. At a given point in time, day 274 of the time simulation, we asked them to look at the completeness values of each of the above mentioned 100 workflows and made them two simple questions: 

1.  Would you reuse this workflow for your own experiments today? and,
2.  Would you use it in three months from now?

Then, we shuffled the workflows and asked them to answer the questions again, this time using the RO Monitoring showing the evolution of the reliability of each workflow until day 274. Then we compare both types of results with the actual behavior of each workflow today and in three months. Two of the users did not pass the control test and were discarded due to the provided answers were outliers. Thus, we focused on the remaining seven for the evaluation. After applying this criterion we made a comparative study between using the completeness and reliability scores, considering the reliability score obtained by the simulation at the end of the evaluating period, three months ahead, as the ground truth. Our results showed that 72% average of the in-the-day reuse decisions (question 1) obtained better results using the reliability score, while this value increased to 76% for question 2. These results are summarized in Table 2. The average improvement distribution for both, question 1 and 2, for each user was 90%, 85%, 90%, 60%, 75%, 77% and 33%, respectively. The complete set of results for the both questions can be seen in tables 3 and 4. 

                          Reuse today   Reuse in 3 months
-----------------------  ------------- -------------------
Better choice (#times)    51            69
Worse choice (#times)     19            22

Table: 2 Reliability vs. Completeness better choice comparative.

Furthermore, the reliability score, and its interpretation through the RO monitoring tool, allow users to make a better job at managing users’ expectations on the convenience of reusing a workflow today or in three months. Based on completeness information alone, 38% workflows would be reused in the day, while incorporating the reliability information constrains this to 32% and even lower (28%) if we ask users to look three months in this future (the complete set of results for all users can be seen at Table 4). 

Overall the use of the reliability score improves significantly the results obtained using completeness information exclusively. In our experiment we have identified a total of 120 cases where the decision of what workflows should and should not be reused improved using reliability values against 41 negative results. This shows evidence that the use of reliability information, based on the record of workflow health over time (completeness assessment), enables scientists to make more informed and better decisions about the reuse of third party scientific workflows, safeguarding their experiments against decay potentially introduced by unstable reused workflows. 

         Better choice (#times)   Worse choice (#times)
------- ------------------------ -----------------------
User 1   10                       1
User 2   7                        1
User 3   9                        1
User 4   6                        4
User 5   10                       4
User 6   6                        2
User 7   3                        6
Total    51                       19

Table: 3 Improvement obtained by using reliability score instead of using completeness score for today question.

         Better choice (#times)   Worse choice (#times)
------- ------------------------ -----------------------
User 1   10                       1
User 2   10                       2
User 3   9                        1
User 4   6                        4
User 5   19                       5
User 6   12                       3
User 7   3                        6
Total    69                       22

Table: 4 Improvement obtained by using reliability score instead of using completeness score for from now in 3 months question.

          Completeness (Today)   Reliability (Today)   Reliability (in 3 months)
-------- ---------------------- --------------------- ---------------------------
User 1    42                     31                    31
User 2    42                     34                    30
User 3    42                     32                    32
User 4    20                     20                    20
User 5    42                     32                    20
User 6    42                     34                    27
User 7    42                     43                    43
-------- ---------------------- --------------------- ---------------------------
Average   37,77 ≈ 38             31,61 ≈ 32            28,09 ≈ 28

Table: 5 Number of times users choose to reuse a workflow based on the completeness and reliability tools for questions 1 and 2. 


### Performance 

We have tested the reliability service in order to obtain the maximum historical data that can be used without having a penalty in the response to end users which would make the interaction slow. It is known in the literature^[http://www.stevenseow.com/papers/UI%20Timing%20Cheatsheet.pdf] that the maximum time of response for instantaneous perception is ≈ 0.1-0.2 secs., and between 0.5-1.0 secs. for immediate perception which does not need to communicate any indication to the user to make him aware that the process is ongoing. Considering these values, we have tested how long backwards, in the historical data of a RO, can be represented using the developed RO monitoring tool to provide a instantaneous or immediate perception to end-users (which turns out to be a response between 0 and 1 sec.).  The table 5 shows the results obtained regarding the time needed to calculate the reliability scores and response for different number of historical days. If we assume a maximum time of 1 sec. as we just mentioned we could provide results until 2 years and 70 days approximately.

 #Days          Response (msecs.)
-------        -------------------
 1               38
 10              42
 30              48
 182             64
 365 (1year)     105
 730 (2years)    912
 1095 (3years)   2355

Table: 6 RO monitoring tool time response for different historical data periods.


## Sustainability and maintainability

For short and mid term, everything developed will be running. The code will be available for developers to download it and use it. The way to contact possible future people in charge of this service is checking the project page or github. There is not an oficial mailing list for this service but main developers will be available to answer questions.

### Survey

**Identity:**

To what extent is the identity of the project/software clear and unique both within its application domain and generally?

* Project/software has its own domain name.
    * Yes.
* Project/software has a logo.
    * No, it uses the project logo.
* Project/software has a distinct name within its application area. A search by Google on the name plus keywords from the application area throws up the project web site in the first page of matches.
    * Yes, it appears on google searches using related keywords.
* Project/software has a distinct name regardless of its application area. A search by Google on the name plus keywords from the application area throws up the project web site in the first page of matches.
    * No, it not appears when using other domain keywords.
* Project/software name does not throw up embarrassing “did you mean...” hits on Google.
    * No, it is generally correct.
* Project/software name does not violate an existing trade-mark.
    * No, it does not violate any trade-mark.
* Project/software name is trade-marked.
    * Yes for the project, not for the specific software.


**Copyright:**

To what extent is it clear who wrote the software and owns its copyright?

* Web site states copyright.
    * Yes.
* Web site states who developed/develops the software, funders etc.
    * Yes, links to Wf4ever.
* If there are multiple web sites then these all state exactly the same copyright, licencing and authorship.
    * Not available.
* Each source code file has a copyright statement.
    * No.
* If supported by the language, each source code file has a copyright statement embedded within a constant.
    * Yes.


**Licencing:**

Has an appropriate licence been adopted?

* Web site states licence.
    * @@TODO
* Software (source and binaries) has a licence.
    * @@TODO
* Software has an open source licence.
    * @@TODO
* Software has an Open Software Initiative (OSI) recognised licence ([http://www.opensource.org/]()).
    * @@TODO
* Each source code file has a licence header.
    * No.


**Governance:**

To what extent does the project make its management, or how its software development is managed, transparent?

* Project has defined a governance policy.
    * No.
* Governance policy is publicly available.
    * Not available.


**Community:**

To what extent does/will an active user community exist for this product?

* Web site has statement of number of users/developers/members.
    * No.
* Web site has success stories.
    * No.
* Web site has quotes from satisfied users.
    * No.
* Web site has list of important partners or collaborators.
    * No, but they can be seen at Wf4ever page.
* Web site has list of the project’s publications.
    * No, but they can be seen at Wf4ever page.
* Web site has list of third-party publications that cite the software.
    * No.
* Web site has list of software that uses/bundles this software.
    * No.
* Users are requested to cite the project if publishing papers based on results derived from the software.
    * No.
* Users are required to cite a boilerplate citation if publishing papers based on results derived from the software.
    * No.
* Users exist who are not members of the project.
    * Yes, there have been some.
* Developers exist who are not members of the project.
    * No.


**Availability:**

To what extent is the software available? (The SSI evaluation uses "accessible" for this quality, but that has been changed here to avoid confusion with other uses of "accessibility")

* Binary distributions are available (whether for free, payment, registration).
    * Not available.
* Binary distributions are freely available.
    * Not available.
* Binary distributions are available without the need for any registration or authorisation of access by the project.
    * Not available.
* Source distributions are available (whether for free, payment, registration).
    * Yes, on github.
* Source distributions are freely available.
    * Yes, on github.
* Source distributions are available without the need for any registration or authorisation of access by the project.
    * Yes, no authorization is needed.
* Access to source code repository is available (whether for free, payment, registration).
    * Yes, on github.
* Anonymous read-only access to source code repository.
    * Yes, on github.
* Ability to browse source code repository online.
    * Yes, on github.
* Repository is hosted externally to a single organisation/institution in a sustainable third- party repository (e.g. SourceForge, GoogleCode, LaunchPad, GitHub) which will live beyond the lifetime of any current funding line.
    * Yes, on github.
* Downloads page shows evidence of regular releases (e.g. six monthly, bi-weekly, etc.).
    * No.


**Testability:**

How straightforward is it to test the software to verify modifications?

* Project has unit tests.
    * Yes, it has base cases to test it.
* Project has integration tests.
    * No.
* For GUIs, project uses automated GUI test frameworks.
    * Not available.
* Project has scripts for testing scenarios that have not been automated (e.g. for testing GUIs).
    * Not available.
* Project recommends tools to check conformance to coding standards.
    * Not available.
* Project has automated tests to check conformance to coding standards.
    * Not available.
* Project recommends tools to check test coverage.
    * Not available.
* Project has automated tests to check test coverage.
    * Not available.
* A minimum test coverage level that must be met has been defined.
    * Not available.
* There is an automated test for this minimum test coverage level.
    * Not available.
* Tests are automatically run nightly.
    * Not available.
* Continuous integration is supported – tests are automatically run whenever the source code changes.
    * Not available.
* Test results are visible to all developers/members.
    * Not available.
* Test results are visible publicly.
    * Not available.
* Test results are e-mailed to a mailing list.
    * Not available.
* This e-mailing list can be subscribed to by anyone.
    * Not available.
* Project specifies how to set up external resources e.g. FTP servers, databases for tests.
    * No.
* Tests create their own files, database tables etc.
    * No.


**Portability:**

To what extent can the software be used on other platforms?

* Application can be built on and run under Windows.
    * Yes.
* Application can be built on and run under Windows 7.
    * Yes.
* Application can be built on and run under Windows XP.
    * Yes.
* Application can be built on and run under Windows Vista.
    * Yes.
* Application can be built on and run under UNIX/Linux.
    * Yes.
* Application can be built on and run under Solaris.
    * Not tested.
* Application can be built on and run under RedHat.
    * Not tested.
* Application can be built on and run under Debian.
    * Yes.
* Application can be built on and run under Fedora.
    * Not tested.
* Application can be built on and run under Ubuntu.
    * Yes.
* Application can be built on and run under MacOSX.
    * Yes.
* Browser applications run under Internet Explorer.
    * Yes.
* Browser applications run under Mozilla Firefox.
    * Yes.
* Browser applications run under Google Chrome.
    * Yes.
* Browser applications run under Opera.
    * Not tested.
* Browser applications run under Safari.
    * Not tested.


**Supportability:**

To what extent will the product be supported currently and in the future?

* Web site has page describing how to get support.
    * No (but common Github tools apply)
* User doc has page describing how to get support.
    * No.
* Software describes how to get support (in a README for command-line tools or a Help=>About window in a GUI).
    * No.
* Above pages/windows/files describe, or link to, a description of “how to ask for help” e.g. cite version number, send transcript, error logs etc.
    * No.
* Project has an e-mail address.
    * No, but see Wf4ever project.
* Project e-mail address has project domain name.
    * Not available.
* E-mails are read by more than one person.
    * Not available.
* E-mails are archived.
    * Not available.
* E-mail archives are publicly readable.
    * Not available.
* E-mail archives are searchable.
    * Not available.
* Project has a ticketing system.
    * Yes, github and JIRA.
* Ticketing system is publicly readable.
    * Only github.
* Ticketing system is searchable.
    * Yes.
* Web site has site map or index.
    * No.
* Web site has search facility.
    * No.
* Project resources are hosted externally to a single organisation/institution in a sustainbable e-mail   archives or ticketing system shows that queries are responded to within a week (not necessarily fixed, but at least looked at and a decision taken as to their priority).
    * Some are hosted externally and some are in Wf4ever project servers.
* If there is a blog, is it is regularly used.
    * Not available.
* E-mail lists or forums, if present, have regular posts.
    * Not available.


**Analysability:**

How straightforward is it to analyse the software’s source release to:
(a) To understand its implementation architecture?
(b) To understand individual source code files and how they fit into the implementation architecture?


* Source code is structured into modules or packages.
    * Yes.
* Source code structure relates clearly to the architecture or design.
    * Yes.
* Project files for IDEs are provided.
    * Yes.
* Source code repository is a revision control system.
    * Yes.
* Structure of the source code repository and how this maps to the software’s components is documented.
    * Part of it.
* Source releases are snapshots of the repository.
    * Yes.
* Source code is commented.
    * Yes.
* Source code comments are written in an API document generation mark-up language e.g. JavaDoc or Doxygen.
    * No.
* Source code is laid out and indented well.
    * Yes.
* Source code uses sensible class, package and variable names.
    * Yes.
* There are no old source code files that should be handled by version control e.g. “SomeComponentOld.java”.
    * There are some that could be needed for other purposes.
* There is no commented out code.
    * Yes, there is. But could be useful.
* There are no TODOs in the code.
    * No. There are no TODOs.
* Coding standards are required to be observed.
    * No.
* Project-specific coding standards are consistent with community or generic coding standards (e.g. for C, Java, FORTRAN etc.).
    * Yes.


**Changeability:**

How straightforward is it to modify the software to:
(a) Address issues?
(b) Modify functionality?
(c) Add new functionality?


* Project has defined a contributions policy.
    * No.
* Contributions policy is publicly available.
    * No.
* Contributors retain copyright/IP of their contributions.
    * Yes (by default; there is no assignment process).
* Users, user-developers and developers who are not project members can contribute.
    * Yes, contributions are welcome.
* Project has defined a stability/deprecation policy for components, APIs etc.
    * No.
* Stability/deprecation policy is publicly available.
    * Not available.
* Releases document deprecated components/APIs in that release.
    * No.
* Releases document removed/changed components/APIs in that release.
    * No.
* Changes in the source code repository are e-mailed to a mailing list.
    * No.
* This e-mailing list can be subscribed to by anyone.
    * Not available.


**Evolvability:**

To what extent will the product be developed in the future:
(a) For a future release?
(b) Within a roadmap for the product?
￼￼

* Web site describes project roadmap or plans or milestones (either on a web page or within a ticketing system).
    * Available at Wf4ever project webpage.
* Web site describes how project is funded/sustained.
    * Available at Wf4ever project webpage.
* We site describes end dates of current funding lines.
    * Available at Wf4ever project webpage.


**Interoperability:**

To what extent does the software’s interoperability:
(a) Meet appropriate open standards?
(b) Function with required third-party components?
(c) Function with optional third-party components?

* Uses open standards.
    * Yes (HTTP, RDF, etc.).
* Uses mature, ratified, non-draft open standards.
    * Yes.
* Provides tests demonstrating compliance to open standards.
    * No.


## Summary of evaluation for stability and reliability service

Scientists, particularly computational scientists, are paying increasing attention to the methods by which scientific results were obtained. Amongst the advantages that this offers, it is worthwhile highlighting some of the following, such as experimental reproducibility and validation, increased trustworthiness as the basis of subsequent research, and, more generally speaking, making science more robust, transparent, pragmatic, and useful.  

The implemented stability and reliability service and its RO monitoring visualization provide a new set of indicators which allow scientist and other end users to take better decisions regarding when to reuse a scientific workflow safeguarding their experiments against decay potentially introduced by unstable reused workflows. This implementation has been developed following the Wf4Ever project standards and also trying to accomplish with some characteristics desired by end-users such as usability (e.g. providing user documentation, or reasonable response times), and by developers such as providing releases information, or code documentation. 

To test the complete implementation and some of the usability desired characteristics we have run some experiments which simulate real scenarios. These simulations have been based on empirical studies which have provided the values for creating the models of the experiments. The results have shown that using these new tool and the calculated indicators, end-users can take better decision whenever they want to reuse a scientific experiment. 

Furthermore we have tested the usability of the interface (RO monitoring tool) regarding the amount of time needed to provide a response. This test has shown that we are currently able to provide a response with an instantaneous or immediate user perception for around 2 years and 2 months of historical data. 

